# H800 æ”¹è¿›ä»»åŠ¡æ‰§è¡Œæ¸…å•

## ç¬¬ä¸€é˜¶æ®µ: ç´§æ€¥ä¿®å¤ (1-2å¤©)

### âœ… H800-New-1: Multi-Horizon LSTM (å·²å®Œæˆ)
- [x] å®ç° `LSTMMultiHorizon` ç±»
- [x] å¤šæ ‡ç­¾è®­ç»ƒå¾ªç¯
- [x] å…ƒæ•°æ®ä¿å­˜
- [ ] **H800 è®­ç»ƒéªŒè¯** â† ä¸‹ä¸€æ­¥
- [ ] å›ä¼ æ¨¡å‹åˆ°æœ¬æœº
- [ ] æ¨ç†ä¾§é›†æˆ

---

### ğŸ”´ H800-Fix-2: TFT è¾“å‡º 0.0 è¯Šæ–­ (ä¼˜å…ˆçº§æœ€é«˜)

**é—®é¢˜**: TFT åœ¨ Stacking ä¸­æƒé‡ä¸ºè´Ÿ (-0.124)ï¼Œå¯èƒ½è¾“å‡ºå¼‚å¸¸

**æ‰§è¡Œæ­¥éª¤**:

1. **æ£€æŸ¥è®­ç»ƒæ—¥å¿—** (10åˆ†é’Ÿ)
```bash
# H800 ä¸ŠæŸ¥çœ‹ TFT è®­ç»ƒæ—¥å¿—
grep -A 20 "TFT" data/gpu_results/train_*.log | tail -50

# æ£€æŸ¥å…³é”®æŒ‡æ ‡:
# - è®­ç»ƒ loss æ˜¯å¦ä¸‹é™
# - éªŒè¯ AUC æ˜¯å¦ > 0.5
# - æ˜¯å¦æœ‰ NaN/Inf
```

2. **å¯¹æ¯” PyTorch vs ONNX è¾“å‡º** (30åˆ†é’Ÿ)
```python
# åœ¨ train_gpu.py æ·»åŠ è¯Šæ–­ä»£ç 
def diagnose_tft_output(model_path, onnx_path, X_test):
    # PyTorch æ¨ç†
    model = TFTModel(...)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    with torch.no_grad():
        pytorch_out = torch.sigmoid(model(X_test)).numpy()

    # ONNX æ¨ç†
    import onnxruntime as ort
    sess = ort.InferenceSession(onnx_path)
    onnx_out = sess.run(None, {'input': X_test.numpy()})[0]

    # å¯¹æ¯”
    print(f"PyTorch output range: [{pytorch_out.min():.4f}, {pytorch_out.max():.4f}]")
    print(f"ONNX output range: [{onnx_out.min():.4f}, {onnx_out.max():.4f}]")
    print(f"Mean absolute diff: {np.abs(pytorch_out - onnx_out).mean():.6f}")
```

3. **æ£€æŸ¥è¾“å…¥å½’ä¸€åŒ–** (20åˆ†é’Ÿ)
```python
# æ£€æŸ¥ ml_live_integration.py ä¸­ TFT çš„é¢„å¤„ç†
# ç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´
```

4. **ä¿®å¤æ–¹æ¡ˆ** (1-2å°æ—¶)
- å¦‚æœ ONNX æœ‰é—®é¢˜: é‡æ–°å¯¼å‡ºæˆ–ä½¿ç”¨ PyTorch æ¨ç†
- å¦‚æœå½’ä¸€åŒ–æœ‰é—®é¢˜: ä¿®æ­£é¢„å¤„ç†ä»£ç 
- å¦‚æœè®­ç»ƒæœ‰é—®é¢˜: è°ƒæ•´è¶…å‚æ•°é‡æ–°è®­ç»ƒ

**é¢„æœŸç»“æœ**: TFT è¾“å‡ºèŒƒå›´ [0.3, 0.7]ï¼ŒStacking æƒé‡å˜æ­£

---

### ğŸŸ¡ H800-Fix-3: Stacking æ¿€æ´»è·¯å¾„è¯Šæ–­ (ä¸­ä¼˜å…ˆçº§)

**é—®é¢˜**: OOF 0.5883 â†’ Test 0.5429 (æ³›åŒ–å·®)

**æ‰§è¡Œæ­¥éª¤**:

1. **æ£€æŸ¥å…ƒå­¦ä¹ å™¨æ¿€æ´»** (15åˆ†é’Ÿ)
```python
# åœ¨ train_gpu.py Stacking è®­ç»ƒéƒ¨åˆ†æ·»åŠ 
from sklearn.linear_model import LogisticRegression

meta_model = LogisticRegression(max_iter=1000, random_state=42)
meta_model.fit(oof_filled, y_oof)

# æ£€æŸ¥: LogisticRegression å†…ç½® sigmoidï¼Œä¸éœ€è¦æ‰‹åŠ¨åº”ç”¨
print(f"Meta model coefficients: {meta_model.coef_}")
print(f"Meta model intercept: {meta_model.intercept_}")

# æµ‹è¯•é¢„æµ‹
test_pred = meta_model.predict_proba(test_base_preds)[:, 1]
print(f"Test pred range: [{test_pred.min():.4f}, {test_pred.max():.4f}]")
```

2. **æ£€æŸ¥æ¨ç†ä¾§æ¿€æ´»** (15åˆ†é’Ÿ)
```python
# åœ¨ ml_predictor.py â†’ StackingPredictor.predict() ä¸­
# ç¡®è®¤ä½¿ç”¨ predict_proba() è€Œé predict()
```

3. **æ·»åŠ å•å…ƒæµ‹è¯•** (30åˆ†é’Ÿ)
```python
# test_stacking_consistency.py
def test_stacking_activation():
    # è®­ç»ƒç®€å• Stacking
    # éªŒè¯è®­ç»ƒå’Œæ¨ç†è¾“å‡ºä¸€è‡´
    pass
```

**é¢„æœŸç»“æœ**: Test AUC æ¥è¿‘ OOF AUC (å·®è· < 0.02)

---

### ğŸŸ¢ H800-Fix-4: è¿‡æ‹Ÿåˆç¼“è§£ (é«˜æ”¶ç›Š)

**æ–¹æ¡ˆ 1: æ•°æ®å¢å¼º** (2å°æ—¶å®ç° + 2å°æ—¶è®­ç»ƒ)

```python
# åœ¨ train_gpu.py æ·»åŠ æ•°æ®å¢å¼ºå‡½æ•°
def augment_time_series(X, y, augment_ratio=0.5):
    """æ—¶é—´åºåˆ—æ•°æ®å¢å¼º"""
    n_aug = int(len(X) * augment_ratio)
    X_aug, y_aug = [], []

    for _ in range(n_aug):
        idx = np.random.randint(len(X))
        x, label = X[idx], y[idx]

        # 1. é«˜æ–¯å™ªå£° (50% æ¦‚ç‡)
        if np.random.rand() < 0.5:
            noise = np.random.normal(0, 0.01, x.shape)
            x = x + noise

        # 2. ç‰¹å¾ Dropout (30% æ¦‚ç‡)
        if np.random.rand() < 0.3:
            mask = np.random.rand(*x.shape) > 0.1
            x = x * mask

        # 3. æ—¶é—´æ‰­æ›² (20% æ¦‚ç‡)
        if np.random.rand() < 0.2:
            # éšæœºæ‹‰ä¼¸/å‹ç¼©æ—¶é—´è½´
            indices = np.sort(np.random.choice(len(x), len(x), replace=True))
            x = x[indices]

        X_aug.append(x)
        y_aug.append(label)

    return np.vstack([X, X_aug]), np.hstack([y, y_aug])

# åœ¨è®­ç»ƒå‰åº”ç”¨
X_train_aug, y_train_aug = augment_time_series(X_train, y_train, augment_ratio=0.3)
```

**æ–¹æ¡ˆ 2: æ­£åˆ™åŒ–å¢å¼º** (30åˆ†é’Ÿ)

```python
# LightGBM
lgb_params = {
    'lambda_l1': 0.05,  # ä» 0.025 å¢åŠ 
    'lambda_l2': 1.0,   # ä» 0.66 å¢åŠ 
    'min_child_samples': 100,  # ä» 52 å¢åŠ 
    'feature_fraction': 0.6,  # ä» 0.67 é™ä½
}

# LSTM
DROPOUT = 0.4  # ä» 0.3 å¢åŠ 
weight_decay = 5e-4  # ä» 1e-4 å¢åŠ 
```

**æ–¹æ¡ˆ 3: Early Stopping ä¸¥æ ¼åŒ–** (15åˆ†é’Ÿ)

```python
# ä½¿ç”¨ holdout set è€Œé validation set
patience = 5  # ä» 10 é™ä½
```

**æ‰§è¡Œé¡ºåº**:
1. å…ˆå®ç°æ•°æ®å¢å¼º (æœ€é«˜æ”¶ç›Š)
2. è®­ç»ƒ LGB + LSTM éªŒè¯æ•ˆæœ
3. å¦‚æœæ•ˆæœå¥½ï¼Œåº”ç”¨åˆ°æ‰€æœ‰æ¨¡å‹

**é¢„æœŸç»“æœ**:
- LGB: Holdout AUC 0.5533 â†’ 0.58
- LSTM: Val AUC 0.5454 â†’ 0.56

---

## ç¬¬äºŒé˜¶æ®µ: æ¨¡å‹å¢å¼º (3-5å¤©)

### ğŸŸ¡ H800-New-1-v2: Multi-Horizon LSTM ä¼˜åŒ–

**å‰ç½®æ¡ä»¶**: H800-New-1 è®­ç»ƒå®Œæˆå¹¶éªŒè¯

**ä¼˜åŒ– 1: Focal Loss** (1å°æ—¶)

```python
# åœ¨ train_gpu.py æ·»åŠ 
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, pred, target):
        bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')
        pt = torch.exp(-bce)
        focal = self.alpha * (1 - pt) ** self.gamma * bce
        return focal.mean()

# æ›¿æ¢ criterion
criterion = FocalLoss(alpha=0.25, gamma=2.0)
```

**ä¼˜åŒ– 2: åŠ æƒæŸå¤±** (30åˆ†é’Ÿ)

```python
# æ ¹æ®æ—¶é—´è·¨åº¦é‡è¦æ€§åŠ æƒ
loss = (focal_loss(pred_5h, yb_5h) * 0.4 +
        focal_loss(pred_12h, yb_12h) * 0.35 +
        focal_loss(pred_24h, yb_24h) * 0.25)
```

**ä¼˜åŒ– 3: å¤´é—´å¤šæ ·æ€§æ­£åˆ™åŒ–** (1å°æ—¶)

```python
def diversity_loss(pred_5h, pred_12h, pred_24h):
    # è®¡ç®—é¢„æµ‹é—´çš„ç›¸å…³æ€§
    preds = torch.stack([pred_5h, pred_12h, pred_24h])
    corr_matrix = torch.corrcoef(preds)

    # æƒ©ç½šé«˜ç›¸å…³æ€§ (é¼“åŠ±å¤šæ ·æ€§)
    off_diag = corr_matrix[torch.triu(torch.ones_like(corr_matrix), diagonal=1) == 1]
    return off_diag.abs().mean()

# æ·»åŠ åˆ°æ€»æŸå¤±
total_loss = task_loss + 0.1 * diversity_loss(pred_5h, pred_12h, pred_24h)
```

**ä¼˜åŒ– 4: é›†æˆæ¨ç†** (30åˆ†é’Ÿ)

```python
# åœ¨æ¨ç†æ—¶åŠ æƒå¹³å‡ 3 ä¸ªå¤´
def forward(self, x, return_all=False):
    out_5h = self.head_5h(context).squeeze(-1)
    out_12h = self.head_12h(context).squeeze(-1)
    out_24h = self.head_24h(context).squeeze(-1)

    if return_all:
        return out_5h, out_12h, out_24h

    # åŠ æƒé›†æˆ (æ ¹æ®éªŒè¯ AUC)
    return 0.4 * out_5h + 0.35 * out_12h + 0.25 * out_24h
```

**æ‰§è¡Œ**: åœ¨ H800 ä¸Šé‡æ–°è®­ç»ƒ

**é¢„æœŸç»“æœ**: Val AUC 0.57 â†’ 0.60+

---

### ğŸŸ¢ H800-New-2: 24h Regime åˆ†ç±»å™¨ (é«˜ä»·å€¼)

**å®ç°** (4å°æ—¶)

```python
# åœ¨ train_gpu.py æ·»åŠ æ–°æ¨¡å¼
def train_regime_classifier(timeframe='1h'):
    """è®­ç»ƒå¸‚åœº Regime åˆ†ç±»å™¨"""

    # 1. å‡†å¤‡æ•°æ®
    features, _ = prepare_features(SYMBOL, timeframe)

    # 2. ç”Ÿæˆ Regime æ ‡ç­¾
    df = load_klines_local(SYMBOL, timeframe)

    # æœªæ¥ 24h æ³¢åŠ¨ç‡
    fwd_vol_24h = df['close'].pct_change().rolling(24).std().shift(-24)

    # ä¸‰åˆ†ä½æ•°åˆ†ç±»
    labels = pd.qcut(fwd_vol_24h, q=3, labels=[0, 1, 2])
    # 0: ä½æ³¢, 1: ä¸­æ³¢, 2: é«˜æ³¢

    # 3. æ¨¡å‹
    class RegimeClassifier(nn.Module):
        def __init__(self, input_dim, hidden_dim=128):
            super().__init__()
            self.lstm = nn.LSTM(input_dim, hidden_dim, 2,
                                batch_first=True, bidirectional=True)
            self.classifier = nn.Sequential(
                nn.Linear(hidden_dim * 2, 64),
                nn.GELU(),
                nn.Dropout(0.3),
                nn.Linear(64, 3),  # 3 classes
            )

        def forward(self, x):
            lstm_out, _ = self.lstm(x)
            return self.classifier(lstm_out[:, -1, :])

    # 4. è®­ç»ƒ
    model = RegimeClassifier(input_dim, hidden_dim=128).to(device)
    criterion = nn.CrossEntropyLoss()
    # ... è®­ç»ƒå¾ªç¯ ...

    # 5. ä¿å­˜
    torch.save(model.state_dict(), 'data/ml_models/regime_classifier_24h.pt')

# åœ¨ main() ä¸­æ·»åŠ 
elif args.mode == 'regime':
    result = train_regime_classifier(args.tf)
```

**è®­ç»ƒ** (2å°æ—¶)

```bash
python3 train_gpu.py --mode regime --tf 1h
```

**é›†æˆåˆ°å®ç›˜** (2å°æ—¶)

```python
# åœ¨ live_config.py ä¸­æ ¹æ® Regime è°ƒæ•´å‚æ•°
regime = regime_classifier.predict(current_features)

if regime == 0:  # ä½æ³¢
    leverage = 10
    stop_loss_pct = 0.02
elif regime == 1:  # ä¸­æ³¢
    leverage = 5
    stop_loss_pct = 0.03
else:  # é«˜æ³¢
    leverage = 2
    stop_loss_pct = 0.05
```

**é¢„æœŸç»“æœ**:
- åˆ†ç±»å‡†ç¡®ç‡ > 60%
- å¤æ™®æ¯”ç‡ +20%

---

### ğŸŸ¡ H800-New-3: 15m LSTM

**å®ç°** (2å°æ—¶)

```python
# ä¿®æ”¹ train_lstm() æ”¯æŒ 15m
# ä¸»è¦è°ƒæ•´:
SEQ_LEN = 192  # 48h å†å² (192 * 15m)
HIDDEN_DIM = 128  # æ¯” 1h å°
label = 'profitable_long_3'  # 3 ä¸ª 15m = 45 åˆ†é’Ÿ
```

**è®­ç»ƒ** (2å°æ—¶)

```bash
python3 train_gpu.py --mode lstm --tf 15m
```

**é¢„æœŸç»“æœ**: Val AUC 0.52-0.54

---

### ğŸŸ¢ H800-New-4: æŸå¤±å‡½æ•°ä¼˜åŒ–

**å·²åœ¨ H800-New-1-v2 ä¸­å®ç° Focal Loss**

**é¢å¤–ä¼˜åŒ–: AUC Loss** (2å°æ—¶)

```python
# éœ€è¦å®‰è£… libauc
# pip install libauc

from libauc.losses import AUCMLoss
from libauc.optimizers import PESG

criterion = AUCMLoss()
optimizer = PESG(model.parameters(), lr=0.1, momentum=0.9)
```

**é¢„æœŸç»“æœ**: AUC +0.01-0.02

---

## ç¬¬ä¸‰é˜¶æ®µ: æ¶æ„å‡çº§ (5-7å¤©)

### ğŸ”µ H800-Arch-1: Transformer æ›¿ä»£ LSTM (é«˜é£é™©é«˜æ”¶ç›Š)

**å®ç°** (8å°æ—¶)

```python
class TransformerPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, num_heads=8, num_layers=4):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1, 200, hidden_dim))

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        # Multi-Horizon è¾“å‡º
        self.head_5h = nn.Linear(hidden_dim, 1)
        self.head_12h = nn.Linear(hidden_dim, 1)
        self.head_24h = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.input_proj(x)
        x = x + self.pos_encoding[:, :x.size(1), :]
        x = self.transformer(x)
        x = x[:, -1, :]  # æœ€åä¸€ä¸ªæ—¶é—´æ­¥

        return self.head_5h(x), self.head_12h(x), self.head_24h(x)
```

**è®­ç»ƒ** (4å°æ—¶)

```bash
python3 train_gpu.py --mode transformer --tf 1h
```

**é£é™©**: è®­ç»ƒä¸ç¨³å®šï¼Œå¯èƒ½éœ€è¦è°ƒå‚

**é¢„æœŸç»“æœ**: Val AUC 0.60+

---

### ğŸ”µ H800-Arch-3: çŸ¥è¯†è’¸é¦ (ä¸­é£é™©ä¸­æ”¶ç›Š)

**å®ç°** (6å°æ—¶)

```python
def train_with_distillation(student_model, teacher_model, X_train, y_train):
    """çŸ¥è¯†è’¸é¦è®­ç»ƒ"""

    # Teacher é¢„æµ‹ (è½¯æ ‡ç­¾)
    teacher_model.eval()
    with torch.no_grad():
        teacher_probs = torch.sigmoid(teacher_model(X_train))

    # Student è®­ç»ƒ
    for epoch in range(EPOCHS):
        student_model.train()
        for xb, yb in train_loader:
            # è·å– teacher è½¯æ ‡ç­¾
            with torch.no_grad():
                teacher_soft = torch.sigmoid(teacher_model(xb))

            # Student é¢„æµ‹
            student_logits = student_model(xb)

            # è’¸é¦æŸå¤±
            loss = distillation_loss(
                student_logits,
                teacher_soft,
                yb,
                alpha=0.5,  # è½¯ç¡¬æ ‡ç­¾æƒé‡
                T=2.0       # æ¸©åº¦
            )

            loss.backward()
            optimizer.step()
```

**é¢„æœŸç»“æœ**: LSTM AUC 0.5454 â†’ 0.57+

---

## æ‰§è¡Œæ—¶é—´è¡¨

| æ—¥æœŸ | ä»»åŠ¡ | é¢„è®¡è€—æ—¶ | è´Ÿè´£äºº |
|------|------|---------|--------|
| Day 1 | H800-Fix-2 (TFT è¯Šæ–­) | 2h | H800 |
| Day 1 | H800-Fix-3 (Stacking è¯Šæ–­) | 1h | H800 |
| Day 1 | H800-Fix-4 (æ•°æ®å¢å¼ºå®ç°) | 2h | H800 |
| Day 1 | H800-New-1 è®­ç»ƒéªŒè¯ | 2h | H800 |
| Day 2 | H800-Fix-4 (é‡æ–°è®­ç»ƒ LGB/LSTM) | 4h | H800 |
| Day 2 | H800-New-1-v2 (Focal Loss) | 2h | H800 |
| Day 2 | H800-New-1-v2 è®­ç»ƒ | 2h | H800 |
| Day 3 | H800-New-2 (Regime å®ç°) | 4h | H800 |
| Day 3 | H800-New-2 è®­ç»ƒ | 2h | H800 |
| Day 3 | H800-New-3 (15m LSTM) | 2h | H800 |
| Day 4 | H800-New-3 è®­ç»ƒ | 2h | H800 |
| Day 4 | H800-Arch-1 (Transformer å®ç°) | 6h | H800 |
| Day 5 | H800-Arch-1 è®­ç»ƒè°ƒè¯• | 8h | H800 |
| Day 6 | H800-Arch-3 (çŸ¥è¯†è’¸é¦) | 6h | H800 |
| Day 7 | æ¨¡å‹å›ä¼  + æ¨ç†é›†æˆ | 4h | æœ¬æœº |

---

## æ£€æŸ¥ç‚¹

### Day 1 ç»“æŸ
- [ ] TFT é—®é¢˜å·²è¯Šæ–­
- [ ] Stacking æ¿€æ´»è·¯å¾„å·²éªŒè¯
- [ ] æ•°æ®å¢å¼ºä»£ç å·²å®ç°
- [ ] Multi-Horizon LSTM è®­ç»ƒå®Œæˆ

### Day 3 ç»“æŸ
- [ ] è¿‡æ‹Ÿåˆé—®é¢˜å·²ç¼“è§£ (Holdout AUC > 0.58)
- [ ] Multi-Horizon LSTM ä¼˜åŒ–å®Œæˆ (Val AUC > 0.57)
- [ ] Regime åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ

### Day 5 ç»“æŸ
- [ ] 15m LSTM è®­ç»ƒå®Œæˆ
- [ ] Transformer æ¨¡å‹è®­ç»ƒå®Œæˆ

### Day 7 ç»“æŸ
- [ ] æ‰€æœ‰æ¨¡å‹å·²å›ä¼ 
- [ ] æ¨ç†ä¾§å·²é›†æˆ
- [ ] å›æµ‹éªŒè¯é€šè¿‡

---

**åˆ›å»ºæ—¶é—´**: 2026-02-20
**é¢„è®¡å®Œæˆ**: 2026-02-27
